\documentclass{article}
\usepackage{geometry}[a4paper, margin=1in]
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{diagbox}
\usepackage{subcaption}
\usepackage{float}
\usepackage{verbatim}

\title{Algorithms for Uncertainty Quantification\\
Bonus Assignment 2}
\author{Onur Yılmaz, 03764831 \\Zafer Bora Yılmazer, 03782277 }
\date{May 2024}

\begin{document}

\maketitle

\section{Lagrange Interpolation}
placeholder

\section{Orthogonal Polynomials}
\subsection{Orthogonal Check by Expectation Value}
By definition of the expectation of a continuous random variable (in our case $\rho$), the expected value is 
\begin{equation}
    \mathbb{E}[\phi_i(x)\cdot\phi_j(x)] = \int_{-\infty}^\infty\phi_i(x)\phi_j(x)\rho(x)dx
\end{equation}
From the definition of orthogonality of two polynomials, two polynomials are orthogonal if 
\begin{equation}
    \int_{-\infty}^\infty\phi_i(x)\phi_j(x)\rho(x)dx = \left\langle\phi_i(x), \phi_j(x)\right\rangle _\rho\delta_{ij}
\end{equation}
Therefore, after combining the two equations we get 
\begin{equation}
    \mathbb{E}[\phi_i(x)\cdot\phi_j(x)] = \left\langle\phi_i(x), \phi_j(x)\right\rangle _\rho\delta_{ij}
\end{equation}
which yields the following insights:
\begin{itemize}
    \item Expected value returns a non-zero value only if $i=j$ due to Kronecker delta.
    \item If the expected value is zero for all other $i$ and $j$ combinations, orthogonality between the two 
    polynomials is revealed.
    \item If the expected value is 1, one can say that the polynomials are orthonormal since 
    \[\int_{-\infty}^\infty\phi_i(x)\phi_j(x)\rho(x)dx = \delta_{ij}\] is satisfied only if the two polynomials are 
    orthonormal. For all other non-zero values, it can be deduced that the polynomials are orthogonal.
\end{itemize}

\subsection{Orthonormal Polynomial Generation}
After implementing the code for orthonormal polynomial generation up to 8 degrees, we got the following matrix for the
expected values $\mathbb{E}[\phi_i(x)\cdot\phi_j(x)]$ for each pair of $\phi_i(x)$ and $\phi_j(x)$ using uniform 
distribution:
\begin{table}[H]
\centering
\makebox[\textwidth]{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
        \hline
        \diagbox{j}{i} & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
        \hline
        1 & 1.000e+00 & 0.000e+00 & 0.000e+00 & 0.000e+00 & 4.441e-16 & 0.000e+00 & -3.553e-15 & 0.000e+00 & 3.553e-15 \\ 
        \hline
        2 & 0.000e+00 & 1.000e+00 & 0.000e+00 & 0.000e+00 & 0.000e+00 & -3.553e-15 & 0.000e+00 & 0.000e+00 & 0.000e+00 \\
        \hline
        3 & 0.000e+00 & 0.000e+00 & 1.000e+00 & 0.000e+00 & -8.882e-16 & 0.000e+00 & -3.553e-15 & 0.000e+00 & 1.421e-14 \\
        \hline
        4 & 0.000e+00 & 0.000e+00 & 0.000e+00 & 1.000e+00 & 0.000e+00 & 3.553e-15 & 0.000e+00 & 1.421e-14 & 0.000e+00 \\
        \hline
        5 & 4.441e-16 & 0.000e+00 & -8.882e-16 & 0.000e+00 & 1.000e+00 & 0.000e+00 & 1.421e-14 & 0.000e+00 & -5.684e-14 \\
        \hline
        6 & 0.000e+00 & -3.553e-15 & 0.000e+00 & 3.553e-15 & 0.000e+00 & 1.000e+00 & 0.000e+00 & 8.527e-14 & 0.000e+00 \\
        \hline
        7 & -3.553e-15 & 0.000e+00 & -3.553e-15 & 0.000e+00 & 1.421e-14 & 0.000e+00 & 1.000e+00 & 0.000e+00 & 4.547e-13 \\
        \hline
        8 & 0.000e+00 & 0.000e+00 & 0.000e+00 & 1.421e-14 & 0.000e+00 & 8.527e-14 & 0.000e+00 & 1.000e+00 & 0.000e+00 \\
        \hline
        9 & 3.553e-15 & 0.000e+00 & 1.421e-14 & 0.000e+00 & -5.684e-14 & 0.000e+00 & 0.000e+00 & 0.000e+00 & 1.000e+00 \\
        \hline
    \end{tabular}
}
\caption{Expectation values for the uniform distribution}\label{tab:uniform}
\end{table}
Similarly for the normal distribution case, we obtained the following results:
\begin{table}[H]
    \centering
    \makebox[\textwidth]{
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
            \hline
            \diagbox{j}{i} & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
            \hline
            1 & 1.000e+00 & 0.000e+00 & 1.421e-14 & 5.684e-14 & 1.819e-12 & -1.455e-11 & -8.731e-11 & 0.000e+00 & -8.149e-10 \\
            \hline
            2 & 0.000e+00 & 1.000e+00 & -2.274e-13 & 5.457e-12 & -1.819e-11 & -4.366e-11 & 2.095e-09 & 3.725e-09 & -4.470e-08 \\
            \hline
            3 & 1.421e-14 & -2.274e-13 & 1.000e+00 & 6.185e-11 & 7.567e-10 & 6.985e-10 & -3.260e-08 & -3.297e-07 & -8.270e-07 \\
            \hline
            4 & 5.684e-14 & 5.457e-12 & 6.185e-11 & 1.000e+00 & -5.006e-09 & -5.960e-08 & -1.863e-07 & -3.397e-06 & -2.146e-05 \\
            \hline
            5 & 1.819e-12 & -1.819e-11 & 7.567e-10 & -1.281e-09 & 1.000e+00 & -6.109e-07 & -3.055e-06 & -2.170e-05 & 9.727e-05 \\
            \hline
            6 & -1.455e-11 & -4.366e-11 & 4.424e-09 & -4.470e-08 & -1.341e-07 & 1.000e+00 & -3.672e-05 & -2.575e-05 & -1.274e-03 \\
            \hline
            7 & -8.731e-11 & 2.095e-09 & -1.770e-08 & 7.674e-07 & -3.055e-06 & -3.672e-05 & 1.000e+00 & 5.493e-04 & -8.484e-03 \\
            \hline
            8 & 0.000e+00 & 3.725e-09 & -9.127e-08 & -5.960e-08 & 3.934e-05 & -1.478e-04 & -1.282e-03 & 1.011e+00 & -6.653e-03 \\
            \hline
            9 & -8.149e-10 & -4.470e-08 & -1.118e-07 & 1.669e-05 & -1.469e-04 & 6.790e-04 & -6.714e-04 & 1.887e-01 & 2.402e-01 \\
            \hline
        \end{tabular}
    }
    \caption{Expectation values for the normal distribution}\label{tab:normal}
    \end{table}
As can be seen in both table \ref{tab:uniform} and \ref{tab:uniform}, diagonal entries (i.e. entries where the Kronecker
delta $\delta_{ij}=1)$ show an expectation value of 1 as expected, while non-diagonal entries report either 0 or values 
with very small order of magnitudes, possibly due to floating point calculations. Only exception is the $i=j=9$ case for 
the normal distribution, where the value is the largest among other elements in the row but still not equal to 1. It can
be attributed to the same reason that there is more non-zero off-diagonal entries in the normal distribution case, yet
we cannot tell the exact reason why.

\section{Probabilistic Colocation}
For fixed $t$, we can show the given mean equality $\mathbb{E}[f(t,\omega)] = \hat{f}_0(t)$ by first showing that 
\begin{equation}\label{eq:f_to_n}
    f(t,\omega) \approx f^N(t,\omega) \Rightarrow \mathbb{E}[f(t,\omega)] \approx \mathbb{E}[f^N(t,\omega)]
\end{equation}
Replacing the $f^N(t,\omega)$ with its summation form yields
\begin{equation}
    \mathbb{E}[f^N(t,\omega)] = \mathbb{E}\left[\sum_{i=0}^{N-1}\hat{f}_i(t)\phi_i(\omega)\right] = \sum_{i=0}^{N-1}\hat{f}_i(t)\mathbb{E}\left[\phi_i(\omega)\right]
\end{equation}
since the term $\hat{f}_i(t)$ is a deterministic value under constant $t$ as seen by the equation 6 of the assignment 
sheet and thus its expected value is equal to itself. Orthonormal polynomials with respect to a 
probability distribution $\rho$ of $\omega$ has the property that only one of the polynomials $\phi_i(\omega)$ can be
1 while all the others have to be zero (as shown in the parts 2.1 and 2.2). Therefore all the terms that do not include
$\phi_0(\omega)$ drop due to them being zero and the summation results in
\begin{equation}
    \sum_{i=0}^{N-1}\hat{f}_i(t)\mathbb{E}\left[\phi_i(\omega)\right] = \hat{f}_0(t)\mathbb{E}\left[\phi_0(\omega)\right] = \hat{f}_0(t)
\end{equation}
With the above equation, we have shown that $\mathbb{E}[f(t,\omega)] = \hat{f}_0(t)$ is approximately equal to $\hat{f}_0(t)$.

For showing the variance, we can start with the following definition of variance for our case
\begin{equation}
    \text{Var}[f(t,\omega)] = \mathbb{E}[f(t, \omega)^2] + \mathbb{E}[f(t,\omega)]^2
\end{equation}
By using the same replacement as we did in \ref*{eq:f_to_n}, we can write the following:
\begin{equation}
    \text{Var}[f(t,\omega)] = \mathbb{E}\left[\left(\sum_{i=0}^{N-1}\hat{f}_i(t)\phi_i(\omega)\right)^2\right] + \mathbb{E}\left[\sum_{i=0}^{N-1}\hat{f}_i(t)\phi_i(\omega)\right]^2
\end{equation}
Second term in the equation above can be found easily from the previously calculated mean as $\hat{f}_0(t)^2$. Focusing
on the first part of the equation yields
\begin{equation}
    \mathbb{E}\left[\left(\sum_{i=0}^{N-1}\hat{f}_i(t)\phi_i(\omega)\right)^2\right] = \sum_{i=0}^{N-1}\sum_{j=0}^{N-1}\hat{f}_i(t)\hat{f}_j(t)\mathbb{E}\left[\phi_i(\omega)\phi_j(\omega)\right]
\end{equation}
Due to orthonormality of the polynomials, $\mathbb{E}\left[\phi_i(\omega)\phi_j(\omega)\right] = \delta_{ij}$ which 
reduces the above equation to
\begin{equation}
    \mathbb{E}\left[\left(\sum_{i=0}^{N-1}\hat{f}_i(t)\phi_i(\omega)\right)^2\right] = \sum_{i=0}^{N-1}\hat{f}_i^2(t)
\end{equation}
as only the terms with $i=j$ will be in the summation. Combining the equation above and the square of the mean, we 
obtain 
\begin{equation}
    \text{Var}[f(t,\omega)] = \left(\sum_{i=0}^{N-1}\hat{f}_i(t)^2\right) - \hat{f}_0^2(t) = \left(\sum_{i=1}^{N-1}\hat{f}_i^2(t)\right)
\end{equation}
\end{document}
