\documentclass{article}
\usepackage{geometry}[a4paper, margin=1in]
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{diagbox}
\usepackage{subcaption}
\usepackage{float}
\usepackage{verbatim}

\title{Algorithms for Uncertainty Quantification\\
Bonus Assignment 2}
\author{Onur Yılmaz, 03764831 \\Zafer Bora Yılmazer, 03782277 }
\date{May 2024}

\begin{document}

\maketitle

\section{Lagrange Interpolation}
From our code implementation, we got the following results for relative errors of Lagrange interpolation and direct 
Monte Carlo sampling and their runtimes respectively for different number of samples and in the case of Lagrange 
interpolation:
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{M} & \textbf{Expectation Error} & \textbf{Variance Error} & \textbf{Runtime (s)} \\ \hline
    10 & 0.01978 & 0.66356 & 0.01346 \\ \hline
    100 & 0.00155 & 0.15514 & 0.12261 \\ \hline
    1000 & 0.00059 & 0.01892 & 1.20413 \\ \hline
    10000 & 0.00032 & 0.01878 & 8.29839 \\ \hline
    \end{tabular}
    \caption{Direct Monte Carlo errors with N number of interpolation points over M number of samples with corresponding runtimes}
    \label{table:direct_mc}
    \end{table}
    
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{N} & \textbf{M} & \textbf{Expectation Error} & \textbf{Variance Error} & \textbf{Runtime (s)} \\ \hline
        2 & 10 & 0.00943 & 0.90278 & 6.938e-05 \\ \hline
        2 & 100 & 0.01715 & 0.76255 & 3.891e-04 \\ \hline
        2 & 1000 & 0.01590 & 0.75875 & 3.292e-03 \\ \hline
        2 & 10000 & 0.01587 & 0.76042 & 3.134e-02 \\ \hline
        5 & 10 & 0.01978 & 0.66358 & 9.513e-05 \\ \hline
        5 & 100 & 0.00155 & 0.15515 & 7.024e-04 \\ \hline
        5 & 1000 & 0.00059 & 0.01891 & 6.888e-03 \\ \hline
        5 & 10000 & 0.00032 & 0.01880 & 6.712e-02 \\ \hline
        10 & 10 & 0.01978 & 0.66356 & 1.428e-04 \\ \hline
        10 & 100 & 0.00155 & 0.15514 & 1.544e-03 \\ \hline
        10 & 1000 & 0.00059 & 0.01892 & 1.562e-02 \\ \hline
        10 & 10000 & 0.00032 & 0.01878 & 1.319e-01 \\ \hline
        20 & 10 & 0.01978 & 0.66356 & 2.592e-04 \\ \hline
        20 & 100 & 0.00155 & 0.15514 & 2.446e-03 \\ \hline
        20 & 1000 & 0.00059 & 0.01892 & 2.426e-02 \\ \hline
        20 & 10000 & 0.00032 & 0.01878 & 2.373e-01 \\ \hline
        \end{tabular}
        \caption{Lagrange Interpolation Errors and Runtimes}
        \label{table:lagrange_combined}
        \end{table}
The values given in the above tables are rounded to 5 decimal digits for viewing, our code also outputs the 
exact values for each expectation and variance values as well. 

As can be seen from the above tables, Lagrange interpolation methods return almost the same expectation and variance 
values for same number of samples with the direct Monte Carlo sampling, in significantly less time. For instance 
in 10000 samples from direct Monte Carlo sampling, execution takes over 8 seconds whereas in Lagrange interpolation 
case, it takes only 0.2 seconds while virtually obtaining the same values and relative errors with the direct Monte
Carlo implementation (while not visible on the table, values are marginally different after 8 or 9 digits after
the decimal point). Only case where visible differences to the variance errors exist for the case of 2 Lagrange 
interpolation points, where the values are higher than the Monte Carlo sampling for same number of samples M and 
do not show much improvement with increasing number of samples. However in all other cases with all combinations
of different interpolation points N and sample points M, values show a similar trend of approaching the reference 
values with increasing number of samples.

\section{Orthogonal Polynomials}
\subsection{Orthogonal Check by Expectation Value}
By definition of the expectation of a continuous random variable (in our case $\rho$), the expected value is 
\begin{equation}
    \mathbb{E}[\phi_i(x)\cdot\phi_j(x)] = \int_{-\infty}^\infty\phi_i(x)\phi_j(x)\rho(x)dx
\end{equation}
From the definition of orthogonality of two polynomials, two polynomials are orthogonal if 
\begin{equation}
    \int_{-\infty}^\infty\phi_i(x)\phi_j(x)\rho(x)dx = \left\langle\phi_i(x), \phi_j(x)\right\rangle _\rho\delta_{ij}
\end{equation}
Therefore, after combining the two equations we get 
\begin{equation}
    \mathbb{E}[\phi_i(x)\cdot\phi_j(x)] = \left\langle\phi_i(x), \phi_j(x)\right\rangle _\rho\delta_{ij}
\end{equation}
which yields the following insights:
\begin{itemize}
    \item Expected value returns a non-zero value only if $i=j$ due to Kronecker delta.
    \item If the expected value is zero for all other $i$ and $j$ combinations, orthogonality between the two 
    polynomials is revealed.
    \item If the expected value is 1, one can say that the polynomials are orthonormal since 
    \[\int_{-\infty}^\infty\phi_i(x)\phi_j(x)\rho(x)dx = \delta_{ij}\] is satisfied only if the two polynomials are 
    orthonormal. For all other non-zero values, it can be deduced that the polynomials are orthogonal.
\end{itemize}

\subsection{Orthonormal Polynomial Generation}
After implementing the code for orthonormal polynomial generation up to 8 degrees, we got the following matrix for the
expected values $\mathbb{E}[\phi_i(x)\cdot\phi_j(x)]$ for each pair of $\phi_i(x)$ and $\phi_j(x)$ using uniform 
distribution:
\begin{table}[H]
\centering
\makebox[\textwidth]{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
        \hline
        \diagbox{j}{i} & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
        \hline
        1 & 1.000e+00 & 0.000e+00 & 0.000e+00 & 0.000e+00 & 4.441e-16 & 0.000e+00 & -3.553e-15 & 0.000e+00 & 3.553e-15 \\ 
        \hline
        2 & 0.000e+00 & 1.000e+00 & 0.000e+00 & 0.000e+00 & 0.000e+00 & -3.553e-15 & 0.000e+00 & 0.000e+00 & 0.000e+00 \\
        \hline
        3 & 0.000e+00 & 0.000e+00 & 1.000e+00 & 0.000e+00 & -8.882e-16 & 0.000e+00 & -3.553e-15 & 0.000e+00 & 1.421e-14 \\
        \hline
        4 & 0.000e+00 & 0.000e+00 & 0.000e+00 & 1.000e+00 & 0.000e+00 & 3.553e-15 & 0.000e+00 & 1.421e-14 & 0.000e+00 \\
        \hline
        5 & 4.441e-16 & 0.000e+00 & -8.882e-16 & 0.000e+00 & 1.000e+00 & 0.000e+00 & 1.421e-14 & 0.000e+00 & -5.684e-14 \\
        \hline
        6 & 0.000e+00 & -3.553e-15 & 0.000e+00 & 3.553e-15 & 0.000e+00 & 1.000e+00 & 0.000e+00 & 8.527e-14 & 0.000e+00 \\
        \hline
        7 & -3.553e-15 & 0.000e+00 & -3.553e-15 & 0.000e+00 & 1.421e-14 & 0.000e+00 & 1.000e+00 & 0.000e+00 & 4.547e-13 \\
        \hline
        8 & 0.000e+00 & 0.000e+00 & 0.000e+00 & 1.421e-14 & 0.000e+00 & 8.527e-14 & 0.000e+00 & 1.000e+00 & 0.000e+00 \\
        \hline
        9 & 3.553e-15 & 0.000e+00 & 1.421e-14 & 0.000e+00 & -5.684e-14 & 0.000e+00 & 0.000e+00 & 0.000e+00 & 1.000e+00 \\
        \hline
    \end{tabular}
}
\caption{Expectation values for the uniform distribution}\label{tab:uniform}
\end{table}
Similarly for the normal distribution case, we obtained the following results:
\begin{table}[H]
    \centering
    \makebox[\textwidth]{
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
            \hline
            \diagbox{j}{i} & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
            \hline
            1 & 1.000e+00 & 0.000e+00 & 1.421e-14 & 5.684e-14 & 1.819e-12 & -1.455e-11 & -8.731e-11 & 0.000e+00 & -8.149e-10 \\
            \hline
            2 & 0.000e+00 & 1.000e+00 & -2.274e-13 & 5.457e-12 & -1.819e-11 & -4.366e-11 & 2.095e-09 & 3.725e-09 & -4.470e-08 \\
            \hline
            3 & 1.421e-14 & -2.274e-13 & 1.000e+00 & 6.185e-11 & 7.567e-10 & 6.985e-10 & -3.260e-08 & -3.297e-07 & -8.270e-07 \\
            \hline
            4 & 5.684e-14 & 5.457e-12 & 6.185e-11 & 1.000e+00 & -5.006e-09 & -5.960e-08 & -1.863e-07 & -3.397e-06 & -2.146e-05 \\
            \hline
            5 & 1.819e-12 & -1.819e-11 & 7.567e-10 & -1.281e-09 & 1.000e+00 & -6.109e-07 & -3.055e-06 & -2.170e-05 & 9.727e-05 \\
            \hline
            6 & -1.455e-11 & -4.366e-11 & 4.424e-09 & -4.470e-08 & -1.341e-07 & 1.000e+00 & -3.672e-05 & -2.575e-05 & -1.274e-03 \\
            \hline
            7 & -8.731e-11 & 2.095e-09 & -1.770e-08 & 7.674e-07 & -3.055e-06 & -3.672e-05 & 1.000e+00 & 5.493e-04 & -8.484e-03 \\
            \hline
            8 & 0.000e+00 & 3.725e-09 & -9.127e-08 & -5.960e-08 & 3.934e-05 & -1.478e-04 & -1.282e-03 & 1.011e+00 & -6.653e-03 \\
            \hline
            9 & -8.149e-10 & -4.470e-08 & -1.118e-07 & 1.669e-05 & -1.469e-04 & 6.790e-04 & -6.714e-04 & 1.887e-01 & 2.402e-01 \\
            \hline
        \end{tabular}
    }
    \caption{Expectation values for the normal distribution}\label{tab:normal}
    \end{table}
As can be seen in both table \ref{tab:uniform} and \ref{tab:uniform}, diagonal entries (i.e. entries where the Kronecker
delta $\delta_{ij}=1)$ show an expectation value of 1 as expected, while non-diagonal entries report either 0 or values 
with very small order of magnitudes, possibly due to floating point calculations. Only exception is the $i=j=9$ case for 
the normal distribution, where the value is the largest among other elements in the row but still not equal to 1. It can
be attributed to the same reason that there is more non-zero off-diagonal entries in the normal distribution case, yet
we cannot tell the exact reason why.

\section{Probabilistic Colocation}
For fixed $t$, we can show the given mean equality $\mathbb{E}[f(t,\omega)] = \hat{f}_0(t)$ by first showing that 
\begin{equation}\label{eq:f_to_n}
    f(t,\omega) \approx f^N(t,\omega) \Rightarrow \mathbb{E}[f(t,\omega)] \approx \mathbb{E}[f^N(t,\omega)]
\end{equation}
Replacing the $f^N(t,\omega)$ with its summation form yields
\begin{equation}
    \mathbb{E}[f^N(t,\omega)] = \mathbb{E}\left[\sum_{i=0}^{N-1}\hat{f}_i(t)\phi_i(\omega)\right] = \sum_{i=0}^{N-1}\hat{f}_i(t)\mathbb{E}\left[\phi_i(\omega)\right]
\end{equation}
since the term $\hat{f}_i(t)$ is a deterministic value under constant $t$ as seen by the equation 6 of the assignment 
sheet and thus its expected value is equal to itself. Orthonormal polynomials with respect to a 
probability distribution $\rho$ of $\omega$ has the property that only one of the polynomials $\phi_i(\omega)$ can be
1 while all the others have to be zero (as shown in the parts 2.1 and 2.2). Therefore all the terms that do not include
$\phi_0(\omega)$ drop due to them being zero and the summation results in
\begin{equation}
    \sum_{i=0}^{N-1}\hat{f}_i(t)\mathbb{E}\left[\phi_i(\omega)\right] = \hat{f}_0(t)\mathbb{E}\left[\phi_0(\omega)\right] = \hat{f}_0(t)
\end{equation}
With the above equation, we have shown that $\mathbb{E}[f(t,\omega)] = \hat{f}_0(t)$ is approximately equal to $\hat{f}_0(t)$.

For showing the variance, we can start with the following definition of variance for our case
\begin{equation}
    \text{Var}[f(t,\omega)] = \mathbb{E}[f(t, \omega)^2] + \mathbb{E}[f(t,\omega)]^2
\end{equation}
By using the same replacement as we did in \ref*{eq:f_to_n}, we can write the following:
\begin{equation}
    \text{Var}[f(t,\omega)] = \mathbb{E}\left[\left(\sum_{i=0}^{N-1}\hat{f}_i(t)\phi_i(\omega)\right)^2\right] + \mathbb{E}\left[\sum_{i=0}^{N-1}\hat{f}_i(t)\phi_i(\omega)\right]^2
\end{equation}
Second term in the equation above can be found easily from the previously calculated mean as $\hat{f}_0(t)^2$. Focusing
on the first part of the equation yields
\begin{equation}
    \mathbb{E}\left[\left(\sum_{i=0}^{N-1}\hat{f}_i(t)\phi_i(\omega)\right)^2\right] = \sum_{i=0}^{N-1}\sum_{j=0}^{N-1}\hat{f}_i(t)\hat{f}_j(t)\mathbb{E}\left[\phi_i(\omega)\phi_j(\omega)\right]
\end{equation}
Due to orthonormality of the polynomials, $\mathbb{E}\left[\phi_i(\omega)\phi_j(\omega)\right] = \delta_{ij}$ which 
reduces the above equation to
\begin{equation}
    \mathbb{E}\left[\left(\sum_{i=0}^{N-1}\hat{f}_i(t)\phi_i(\omega)\right)^2\right] = \sum_{i=0}^{N-1}\hat{f}_i^2(t)
\end{equation}
as only the terms with $i=j$ will be in the summation. Combining the equation above and the square of the mean, we 
obtain 
\begin{equation}
    \text{Var}[f(t,\omega)] = \left(\sum_{i=0}^{N-1}\hat{f}_i(t)^2\right) - \hat{f}_0^2(t) = \left(\sum_{i=1}^{N-1}\hat{f}_i^2(t)\right)
\end{equation}
\end{document}
